{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R03_image_classification_with_cnns.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vasWnqRgy1H4"
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnFU-5KjbkAf"
      },
      "source": [
        "# Notice\n",
        "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
        "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
        "*   Edit -> Clear all outputs\n",
        "*   Runtime -> Reset all runtimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Image Classification with Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R03_image_classification_with_cnns.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R03_image_classification_with_cnns.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n",
        "\n",
        "It's okay if you don't understand everything. This is a fast-paced overview of a complete TensorFlow program, with explanations along the way. The goal is to get the general sense of a TensorFlow project, not to catch every detail.\n",
        "\n",
        "This tutorial uses the [keras](https://keras.rstudio.com/) R package, a high-level API to build and train models in TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0tMfX2vR0uD"
      },
      "source": [
        "## Install and import dependencies\n",
        "\n",
        "We'll need to install the `keras` R package which also includes the example datasets we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2krUp_bhg59b"
      },
      "source": [
        "install.packages(\"keras\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz7nYfD_0plK"
      },
      "source": [
        "Once installed, load the library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAOU9saf0hsf"
      },
      "source": [
        "library(keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YWy-doINtM-"
      },
      "source": [
        "Let's also install a few more packages we will need, mainly for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKLrCghNsG6"
      },
      "source": [
        "install.packages(c(\"ggplot2\", \"cowplot\", \"tidyr\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "This tutorial uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 $\\times$ 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n",
        "\n",
        "This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from the [Keras R package](https://keras.rstudio.com/reference/dataset_fashion_mnist.html) using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toaB9k0X05Vx"
      },
      "source": [
        "fashion_mnist <- dataset_fashion_mnist()\n",
        "str(fashion_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60-A6csz1Bs9"
      },
      "source": [
        "Loading the dataset returns a list of length 2 containing the training (`train`) and testing (`test`) datasets. \n",
        "* The model is trained using `train` dataset.\n",
        "* The model is tested against `test` dataset.\n",
        "\n",
        "Each dataset contains images (`x`) and their associated labels (`y`). We can separate our data into appropriately named objects to work with using the `%<-%` assignment operator.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd1i29tR1CQ2"
      },
      "source": [
        "c(train_images, train_labels) %<-% fashion_mnist$train\n",
        "c(test_images, test_labels) %<-% fashion_mnist$test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8dXuWz4Efzy"
      },
      "source": [
        "The images are 28 $\\times$ 28 arrays, with pixel values in the range `[0, 255]`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjT5-v1j1cPY"
      },
      "source": [
        "str(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJO9yCpJEspq"
      },
      "source": [
        "The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* of clothing the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbUmZpWkEuJh"
      },
      "source": [
        "str(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtO-Ajg2E-NK"
      },
      "source": [
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIcA56p6E_Kl"
      },
      "source": [
        "class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',  'Shirt',   'Sneaker',  'Bag',   'Ankle boot')\n",
        "num_classes <- length(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWQ0N6FZ4jAi"
      },
      "source": [
        "dim(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx5fzEKhBAnt"
      },
      "source": [
        "The pixel values across all images range between 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUKI9FWjITUz"
      },
      "source": [
        "range(train_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihlmQ5J4itA"
      },
      "source": [
        "Likewise, there are 60,000 labels in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1JqmCyxAAHT"
      },
      "source": [
        "dim(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhWuWmklAJ0X"
      },
      "source": [
        "Each label is an integer between 0 and 9:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfZ9RgucALfZ"
      },
      "source": [
        "train_labels %>% unique() %>% sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdDk7WVVBPqD"
      },
      "source": [
        "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va-GAS5wBSkH"
      },
      "source": [
        "dim(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGN1cRPXBfI7"
      },
      "source": [
        "And the test set contains 10,000 images labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YmGbSGKBYqh"
      },
      "source": [
        "dim(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "The data must be preprocessed before training the network. The value of each pixel in the image data is an integer in the range `[0,255]`. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3c9zam7H9Wk"
      },
      "source": [
        "library(tidyr)\n",
        "library(ggplot2)\n",
        "\n",
        "plot_fashionmnist_image <- function(image){\n",
        "  image <- as.data.frame(image)\n",
        "  colnames(image) <- seq_len(ncol(image))\n",
        "  image$y <- seq_len(nrow(image))\n",
        "  image <- gather(image, \"x\", \"value\", -y)\n",
        "  image$x <- as.integer(image$x)\n",
        "\n",
        "  ggplot(image, aes(x = x, y = y, fill = value)) +\n",
        "    geom_tile() +\n",
        "    scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\n",
        "    scale_y_reverse() +\n",
        "    theme_minimal() +\n",
        "          theme(panel.grid = element_blank(), \n",
        "                axis.text = element_blank(),\n",
        "                axis.title = element_blank(),\n",
        "                aspect.ratio = 1)\n",
        "}\n",
        "\n",
        "plot_fashionmnist_image(train_images[1,,])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQBjqhlcQUB"
      },
      "source": [
        "For the model to work properly, these values need to be normalized to the range `[0,1]`. To perform the conversion we can divide each array by 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ksdltMc-2J9"
      },
      "source": [
        "train_images <- train_images/255\n",
        "test_images <- test_images/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynERlg8g7YoX"
      },
      "source": [
        "The Convolution 2D layers we're going to be using expect a 4 dimensional array (tensor) as input to accomodate the notion of images having channels. In our case, our images are grayscale and therefore only have one channel in the channel dimension. Our data must nonetheless be reshaped to shape `(samples, rows, cols, channels)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRltGNI3zwxq"
      },
      "source": [
        "train_images <- array_reshape(train_images, c(nrow(train_images), 28, 28, 1))\n",
        "test_images <- array_reshape(test_images, c(nrow(test_images), 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCsSbDQHBxJV"
      },
      "source": [
        "### Explore the processed data\n",
        "\n",
        "We can check that pixel values have been successfully processed by checking the range of values in each array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8O2zsxVA7R6"
      },
      "source": [
        "range(train_images)\n",
        "range(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIQbEiJGXM-q"
      },
      "source": [
        "To verify that the data appears correctly labelled, let's also display the first 25 images from the training set as well as the class name below each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOPXPfd3a7zc"
      },
      "source": [
        "par(mfcol=c(5,5))\n",
        "par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\n",
        "for (i in 1:25) { \n",
        "  img <- train_images[i, , , ]\n",
        "  img <- t(apply(img, 2, rev)) \n",
        "  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n",
        "        main = paste(class_names[train_labels[i] + 1]))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGtY-MmAG-gv"
      },
      "source": [
        "Let's also check we have the correct dimensions in our data for training, which should be 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoPWeN-k3piI"
      },
      "source": [
        "dim(train_images)\n",
        "dim(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KvH_wY3dw9v"
      },
      "source": [
        "We’re ready to build and train the network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Exercise 3.1 Setup the layers (10 mins)\n",
        "\n",
        "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
        "\n",
        "Much of deep learning consists of chaining together simple layers. Most layers, like Keras `layer_dense`, have internal parameters which are adjusted (\"learned\") during training.\n",
        "\n",
        "For this exercise, we'll be using two new layers, the Convolution layer (`layer_conv_2d`) and the Max Pooling layer (`layer_max_pooling_2d`). Refer to the slides and official documentation on how to use these layers:\n",
        "\n",
        "* [`layer_conv_2d()` reference](https://keras.rstudio.com/reference/layer_conv_2d.html): A 2D convolution layer\n",
        "* [`layer_max_pooling_2d` reference](https://keras.rstudio.com/reference/layer_max_pooling_2d.html): 2D Max pooling operation for spatial data\n",
        "\n",
        "See [Keras R package reference page](https://keras.rstudio.com/reference/index.html#section-core-layers) for more details on all layers \n",
        "\n",
        "<br>\n",
        "\n",
        "**The network layers we want and their configurations are:**\n",
        "* 2D Convolution layer - `c(28, 28, 1)` input shape, 32 filters, 3x3 kernel, ReLU activation, padding with same values\n",
        "* Max pooling layer - 2x2 kernel, 2 stride\n",
        "* 2D Convolution layer - 64 filters, 3x3 kernel, ReLU activation, padding with same values\n",
        "* Max pooling layer - 2x2 kernel, 2 stride\n",
        "* Flatten layer\n",
        "* Dense layer - 128 nodes output, ReLU activation\n",
        "* Dense layer - 10 nodes output, softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLs7IToHyRZZ"
      },
      "source": [
        "#TODO - Create model and add model layers as described above\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9IeSEn_yRZZ"
      },
      "source": [
        "#### Exercise 3.1 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.3.1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUEK5IoRYyX"
      },
      "source": [
        "**What is each layer doing?**\n",
        "\n",
        "* **\"convolutions\"** `layer_conv_2d` and `max_pooling_2d`— Network start with two pairs of Conv/MaxPool layers. The first layer is a `conv_2d` layer with (3,3) filters being applied to the input image, retaining the original image size by using padding, and creating 32 output (convoluted) images (so this layer creates 32 convoluted images of the same size as input). After that, the 32 outputs are reduced in size using a `max_pooling_2d` (2,2) with a stride of 2. The next `conv_2d` also has a (3,3) kernel, takes the 32 images as input and creates 64 outputs which are again reduced in size by a `max_pooling_2d` layer. So far in the course, we have described what a Convolution does, but we haven't yet covered how you chain multiples of these together. We will get back to this in lesson 4 when we use color images. At this point, it's enough if you understand the kind of operation a convolutional filter performs\n",
        "\n",
        "* **output** `layer_dense` — A 128-neuron layer, followed by 10-node *softmax* layer. Each node in the final layer represents a class of clothing. The final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
        "\n",
        "\n",
        "### Exercise 3.2 Compile the model with `compile` (15 mins for 3.2, 3.3 & 3.4)\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
        "\n",
        "\n",
        "**Compile the model below with the following settings**\n",
        "* *Loss function* — 'sparse_categorical_crossentropy'\n",
        "* *Optimizer* — 'adam'\n",
        "* *Metrics* — 'accuracy'\n",
        "\n",
        "Refer to the [official documentation](https://keras.rstudio.com/reference/compile.keras.engine.training.Model.html) if you've forgotten the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DxT9UxaR9zt"
      },
      "source": [
        "# TODO - Compile the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZgfyDVCR3EW"
      },
      "source": [
        "#### Exercise 3.2 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.3.2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IuwUQsE-9Wx"
      },
      "source": [
        "## Exercise 3.3 Train the model with `fit`\n",
        "\n",
        "\n",
        "Training is again performed by calling the `fit()` method:\n",
        "1. Feed the training data to the model using `train_images` & `train_labels`.\n",
        "2. The model learns to associate images and labels.\n",
        "3. The `epochs` parameter limits training to the number of iterations over the whole dataset specified.\n",
        "\n",
        "\n",
        "Start training the model in the code box below for **10 epochs** and using a batch size of 32. Also set the argument `validation_split = 0.2`.  This will retain 20% of our training data for validation purposes and will not be used for testing.\n",
        "\n",
        "Refer to the [documentation](https://keras.rstudio.com/reference/fit.keras.engine.training.Model.html) if you've forgotten the the function or to check default settings.\n",
        "\n",
        "Remember to assign the output of `fit()` to a `history` object. Print `history` out to see metrics about model training and `plot(history)` to get a plot of model performance during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEnkMnDYyRZb"
      },
      "source": [
        "# TODO - Train the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.97 (or 97%) on the training data. However, we see that accuracy on validation data is again lower (0.91) and appears to be diverging as training proceeds, indicating that the model is starting to overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y6MAmT8yRZb"
      },
      "source": [
        "### Exercise 3.3 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.3.3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEw4bZgGCaXB"
      },
      "source": [
        "## Exercise 3.4 Evaluate accuracy with `evaluate`\n",
        "\n",
        "Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy. Print out the accuracy and loss of the model on the test data.\n",
        "\n",
        "Refer to the [documentation](https://keras.rstudio.com/reference/evaluate.keras.engine.training.Model.html) on how to use the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_FPSx0PyRZc"
      },
      "source": [
        "# TODO - Evaluate the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHTJwk0FyRZc"
      },
      "source": [
        "### Exercise 3.4 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.3.4.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbOkv_kg4Jgu"
      },
      "source": [
        "As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6j6ip884Yg2"
      },
      "source": [
        "## Make predictions and explore\n",
        "\n",
        "With the model trained, we can use it to make predictions about some images. Let's subset the first 32 images and labels from the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPASuh8r-X8b"
      },
      "source": [
        "pred_images <- test_images[1:32 , , , , drop = FALSE]\n",
        "pred_labels <- test_labels[1:32]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofyosPN0Rz6T"
      },
      "source": [
        "Next we can use use our model and the keras `predict` function to generate some predictions for our image subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZaXW2FA-uLF"
      },
      "source": [
        "preds_probs <- model %>%\n",
        "  predict(x = pred_images)\n",
        "\n",
        "head(preds_probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osSUnZheR3XU"
      },
      "source": [
        "Here, the model has calculated the probability associated with each label for each image in our subset and returns a matrix with 10 columns (the number of classes) and 32 rows (the number of images in our subset. Note that we have only printed out the top 6 rows above).\n",
        "\n",
        "To get a class prediction we select the class (column) with the highest probability. We can do that by applying the `which.max` function to each row of our prediction matrix. We also need to subtract 1 from each returned column index because while column indexes are 1 indexed (the first column number is indexed by 1), our classes are 0 indexed (the first class in our data is indexed with 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPK1vkzpQkVL"
      },
      "source": [
        "apply(preds_probs, 1, which.max) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5NRlUgfTgH5"
      },
      "source": [
        "This prediction approach can be useful if you want to know the probability distribution for each prediction across all classes. However, if you are only interested in the predicted class, there is a shorthand keras function that can return predicted classes instead, `predict_classes`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUh5_bLqAb2g"
      },
      "source": [
        "preds_classes <- model %>%\n",
        "  predict_classes(x = pred_images)\n",
        "\n",
        "preds_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RXmTn3DUTpW"
      },
      "source": [
        "Let's take a look at the first prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3q-M02DUVa9"
      },
      "source": [
        "preds_classes[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AknTqpncUvsV"
      },
      "source": [
        "Let's see what class that is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv_o0P3TUwzM"
      },
      "source": [
        "class_names[preds_classes[1] + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug0h1rMNUfaH"
      },
      "source": [
        "Our model predicts that this image is an **Ankle boot**, or `class_names[10]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwDVYy-wPhpv"
      },
      "source": [
        "We can graph this to look at the full set of 10 class predictions. First let's write some additional plotting functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWP9HNDev-0"
      },
      "source": [
        "library(ggplot2)\n",
        "library(cowplot)\n",
        "\n",
        "# Function to plot bar plot of probabilities across each class\n",
        "plot_preds_bar <- function(probs, label){\n",
        "\n",
        "  plot_data <- data.frame(class = as.factor(0:9), probability = probs, pred_label = FALSE, label = FALSE)\n",
        "  plot_data$pred_label[which.max(probs)] <- TRUE\n",
        "  plot_data$label[label + 1] <- TRUE\n",
        "  plot_data$legend <- \"not predicted - correct\"\n",
        "  plot_data$legend[plot_data$pred_label & plot_data$label] <- \"predicted - correct\"\n",
        "  plot_data$legend[plot_data$pred_label & !plot_data$label] <- \"predicted - incorrect\"\n",
        "  plot_data$legend[plot_data$label & !plot_data$pred_label] <- \"actual - incorrect\"\n",
        "\n",
        "  ggplot(plot_data, aes(x = class, y = probability, fill = legend)) + \n",
        "    geom_bar(stat = \"identity\") +\n",
        "    scale_fill_manual(values = c(\"predicted - correct\" = \"blue\",\n",
        "                                 \"predicted - incorrect\" = \"red\",\n",
        "                                 \"actual - incorrect\" = \"blue\",\n",
        "                                 \"not predicted - correct\" = \"grey\"),\n",
        "                      guide = \"none\") +\n",
        "    theme_classic()\n",
        "}\n",
        "# Function to plot raw image and bar plot of probabilities across each class\n",
        "plot_preds <- function(image, probs, label, class_names){\n",
        "    \n",
        "    pred_label <- which.max(probs) - 1\n",
        "    correct_label <- label == pred_label\n",
        "    title_colour <- if(correct_label){\"blue\"}else{\"red\"}\n",
        "\n",
        "    # create title string\n",
        "    title <- ggdraw() +\n",
        "      draw_label(\n",
        "        paste0(class_names[pred_label + 1], \" \", format(max(probs) * 100, digits = 3), \n",
        "               \"% \", \" (\", class_names[label + 1], \")\"),\n",
        "        fontface = 'bold',\n",
        "        x = 0,\n",
        "        hjust = 0,\n",
        "        color = title_colour) +\n",
        "      theme(plot.margin = margin(0, 0, 0, 7))\n",
        "    \n",
        "    # Generate the two plots\n",
        "    p1 <- plot_fashionmnist_image(image)\n",
        "    p2 <- plot_preds_bar(probs, label)\n",
        "\n",
        "   # Create row of plots\n",
        "   plot_row <- plot_grid(p1, p2)\n",
        "\n",
        "   # Bring together title and row of images\n",
        "   plot_grid(title, plot_row, ncol = 1, rel_heights = c(0.1, 1))\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2r2AqIrPRIf"
      },
      "source": [
        "options(repr.plot.width = 10, repr.plot.height = 5)\n",
        "i <- 1\n",
        "plot_preds(pred_images[i,,,], preds_probs[i,], pred_labels[i], class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7KiEpduQ5hT"
      },
      "source": [
        "i <- 5\n",
        "plot_preds(pred_images[i,,,], preds_probs[i,], pred_labels[i], class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiCR0dX_NpJF"
      },
      "source": [
        "options(repr.plot.width = 14, repr.plot.height = 8)\n",
        "  j <- 16\n",
        "  plot_list <- as.list(vector(length = j))\n",
        "  for(i in 1:j){\n",
        "    plot_list[[i]]<- plot_preds(pred_images[i,,,], preds_probs[i,], \n",
        "                                pred_labels[i], class_names)\n",
        "  }\n",
        "  plot_grid(plotlist = plot_list, ncol = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8z6xc6TIAxJ"
      },
      "source": [
        "# Exercise 3.5\n",
        "\n",
        "Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n",
        "*   Set training epochs to 1\n",
        "*   Number of neurons in the Dense layer following the Flatten one. For example, go really low (e.g. 10) and then high, up to 512 and see how accuracy changes\n",
        "*   Add additional Dense layers between the Flatten and the final Dense(10), experiment with different units in these layers\n",
        "*   Don't normalize the pixel values, and see the effect that has\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYovvwfQbkAZ"
      },
      "source": [
        "\n",
        "# Exercise 3.6 - CIFAR-10 Dataset (10 mins)\n",
        "\n",
        "Let's apply what we've learned to another dataset.The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "As our input is a colour image, we have now 3 values per pixel. When flattened, our input array is is 3072 long ($32\\times32\\times3$). \n",
        "\n",
        "* What happens when you use the same network as above?\n",
        "* What is the best accuracy that you can achieve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQjFzS02bkAZ"
      },
      "source": [
        "Like in the previous lab, download, extract and load the dataset.\n",
        "\n",
        "The dataset is also available through the `Keras R package`. We can load it using [**`dataset_cifar10()`**](https://keras.rstudio.com/reference/dataset_cifar10.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPhU1aQol_gK"
      },
      "source": [
        "cifar10 <- dataset_cifar10()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9PrvgtTsFJ3"
      },
      "source": [
        "Let's inspect the data. It's similar in structure to the fashion MNIST dataset but the images (`x`) are now 4 dimensional arrays, with a dimension for channels (RGB). The size of the images is 32 $\\times$ 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgTRraChmPpo"
      },
      "source": [
        "str(cifar10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXkMLpJyslFh"
      },
      "source": [
        "The number of labels is again 10 and each label (0-9) maps on to one of the following classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsyktAJZnsUB"
      },
      "source": [
        "cifar_labels <- c('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        " 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMeRw117yRZj"
      },
      "source": [
        "**Now that we've got a dataset, use what you've learned in this lab to build a CNN model for classifying these images and evaluate it with the test data.**\n",
        "\n",
        "Remember the difference in **image shape and number of channels** in the CIFAR-10 dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xWyazYNyRZj"
      },
      "source": [
        "# TODO - Create a CNN model and train it using the CIFAR-10 dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wOEf1oOyRZj"
      },
      "source": [
        "### Exercise 3.6 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.3.6.ipynb)"
      ]
    }
  ]
}