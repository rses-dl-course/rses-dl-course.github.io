{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R06_saving_and_loading_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_tvPdyfA-BL"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0O_LFhwSBCjm"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3Pry4jh1-E"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R06_saving_and_loading_models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R06_saving_and_loading_models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxjpzKTvg_dd"
      },
      "source": [
        "# Lab 06: Saving and Loading Models\n",
        "\n",
        "In this lab we will learn how we can take a trained model, save it, and then load it back to keep training it or use it to perform inference. In particular, we will use transfer learning to train a classifier to classify images of cats and dogs, just like we did in the previous lesson. We will then take our trained model and save it as an HDF5 file, which is the format used by Keras. We will then load this model, use it to perform predictions, and then continue to train the model. Finally, we will save our trained model as a TensorFlow SavedModel and then we will download it to a local disk, so that it can later be used for deployment in different platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crU-iluJIEzw"
      },
      "source": [
        "## Concepts that will be covered in this Colab\n",
        "\n",
        "1. Saving models in HDF5 format for Keras\n",
        "3. Loading models\n",
        "\n",
        "Before starting this Colab, you should reset the Colab environment by selecting `Runtime -> Reset all runtimes...` from menu above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RVsYZLEpEWs"
      },
      "source": [
        "# Install and load dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUCEcRdhnyWn"
      },
      "source": [
        "First, you'll need to install and load R package Keras which will also install TensorFlow. We'll also install package fs which has useful functionality for working with our filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0rPA7CkF5l8"
      },
      "source": [
        "install.packages(c(\"keras\", \"fs\"))\n",
        "library(keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8P2MUDrIUu"
      },
      "source": [
        "# Part 1: Load the Cats vs. Dogs Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9crYcwgzrH9A"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We download the dataset again. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFpoMDvrTTLN"
      },
      "source": [
        "URL <- \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "zip_dir <- get_file('cats_and_dogs_filterted.zip', origin = URL, extract = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKNVlt1MTbEf"
      },
      "source": [
        "The dataset we have downloaded has the following directory structure.\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>\n",
        "\n",
        "We can list the directories with the following terminal command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR3B2TqRV_Pr"
      },
      "source": [
        "zip_dir_base <- dirname(zip_dir)\n",
        "fs::dir_tree(zip_dir_base, recurse = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KnMK35BWGqC"
      },
      "source": [
        "We'll now assign variables with the proper file path for the training and validation sets. We'll also create some variables that hold information about the size of our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlP-iXwoWHRO"
      },
      "source": [
        "base_dir <- fs::path(zip_dir_base, \"cats_and_dogs_filtered\")\n",
        "train_dir <- fs::path(base_dir, \"train\")\n",
        "validation_dir <- fs::path(base_dir, \"validation\")\n",
        "\n",
        "train_cats_dir <- fs::path(train_dir, \"cats\")\n",
        "train_dogs_dir <- fs::path(train_dir, \"dogs\")\n",
        "validation_cats_dir <- fs::path(validation_dir, \"cats\")\n",
        "validation_dogs_dir <- fs::path(validation_dir, \"dogs\")\n",
        "\n",
        "\n",
        "num_cats_tr <- length(fs::dir_ls(train_cats_dir))\n",
        "num_dogs_tr <- length(fs::dir_ls(train_dogs_dir))\n",
        "\n",
        "num_cats_val <- length(fs::dir_ls(validation_cats_dir))\n",
        "num_dogs_val <- length(fs::dir_ls(validation_dogs_dir))\n",
        "\n",
        "total_train <- num_cats_tr + num_dogs_tr\n",
        "total_val <- num_cats_val + num_dogs_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsRhwiQ8rkQv"
      },
      "source": [
        "Lets create training and validation image generators to read our images from their directories which rescales our image to values from 0 to 1. Let's also create a flow from our training directories which also resizes our images to the resolution (224 x 224) expected by our MobileNetV2 model we'll be using for transfer learning. Let's also set the batch size to 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEtvEPm6r8hO"
      },
      "source": [
        "batch_size <- 32\n",
        "image_res <- 224\n",
        "\n",
        "# training generators\n",
        "train_image_generator <- image_data_generator(rescale = 1/255,\n",
        "                                              rotation_range = 45,\n",
        "                                              width_shift_range = 0.2,\n",
        "                                              height_shift_range = 0.2,\n",
        "                                              shear_range = 0.2,\n",
        "                                              zoom_range = 0.2,\n",
        "                                              horizontal_flip = TRUE,\n",
        "                                              fill_mode = 'nearest')\n",
        "\n",
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(image_res, image_res),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)\n",
        "\n",
        "# validation generators\n",
        "val_image_generator <- image_data_generator(rescale = 1/255)\n",
        "\n",
        "val_data_gen <- flow_images_from_directory(directory = validation_dir,\n",
        "                                             generator = val_image_generator,\n",
        "                                             target_size = c(image_res, image_res),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41zDiZdKsYvo"
      },
      "source": [
        "# Part 2: Transfer Learning with TensorFlow Hub\n",
        "\n",
        "\n",
        "Let's now use the [MobileNet V2 model architecture](https://keras.rstudio.com/reference/application_mobilenet_v2.html) to do Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAc_Viz0siWC"
      },
      "source": [
        "feature_extractor <- application_mobilenet_v2(input_shape = c(image_res, image_res, 3),\n",
        "                                              include_top = FALSE, pooling = \"avg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn3zRO2Cslpq"
      },
      "source": [
        "Freeze the variables in the feature extractor layer, so that the training only modifies the final classifier layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqMWwpNzsqv9"
      },
      "source": [
        "freeze_weights(feature_extractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk_WOWlms0Vy"
      },
      "source": [
        "Now let's use are feature extractor as part of a keras sequential model, and add a new classification layer of 2 units with softmax activation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od6ur8sdswqO"
      },
      "source": [
        "model <- keras_model_sequential() %>%\n",
        "          feature_extractor() %>%\n",
        "          layer_dense(units = 2, activation = \"softmax\")\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhT9p4viV_tX"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We now train this model like any other using an image generator, by first calling `compile` followed by `fit_generator`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfZYgHQrT6vD"
      },
      "source": [
        "model %>% compile(optimizer=\"adam\",\n",
        "                  loss = loss_sparse_categorical_crossentropy,\n",
        "                  metrics = \"accuracy\")\n",
        "\n",
        "epochs = 3\n",
        "history <- model %>%\n",
        "              fit_generator(generator = train_data_gen,\n",
        "              steps_per_epoch = as.integer(total_train / batch_size),\n",
        "              epochs = epochs,\n",
        "              validation_data = val_data_gen,\n",
        "              validation_steps = as.integer(total_val / batch_size))\n",
        "\n",
        "cat('Validation loss:', format(tail(history$metrics$val_loss, 1), digits = 2), \"\\n\")\n",
        "cat('Validation accuracy:', format(tail(history$metrics$val_accuracy, 1), digits = 2), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNwjmjaItOM7"
      },
      "source": [
        "## Check the predictions\n",
        "\n",
        "To check our model, let's use it to make some predictions on the images from a single batch of our validation image generator. \n",
        "\n",
        "Let's compile our predictions into a data.frame. Let's also add the actual labels for each row to the data.frame and have a look at our predictions. We can see that predictions using our transfer learning very accurate with very high confidence in each prediciton!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5Hv2-9hteps"
      },
      "source": [
        "pred_batch <- val_data_gen[1]\n",
        "\n",
        "pred_mat <- model %>%\n",
        "  predict(pred_batch[[1]]) \n",
        "\n",
        "predictions <- data.frame(class_description = names(val_data_gen$class_indices)[apply(pred_mat, 1, which.max)],\n",
        "                          score = apply(pred_mat, 1, max),\n",
        "                          label = names(val_data_gen$class_indices)[pred_batch[[2]] + 1])\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNgGWQLKIowr"
      },
      "source": [
        "Let's now plot the images from our Dogs vs Cats dataset and put the predicted labels above them and their actual labels below.  First we create a plotting function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMSo5SUpD0wE"
      },
      "source": [
        "plot_rgb_image_ttl <- function(image_array, prediction){\n",
        "  image_array %>%\n",
        "  array_reshape(dim = c(dim(.)[1:3])) %>%\n",
        "  as.raster(max = 1) %>%\n",
        "  plot()\n",
        "  title(main = paste0(prediction$class_description, \n",
        "                      \" (\", format(prediction$score * 100, digits = 2), \"%)\"),\n",
        "        sub = prediction$label)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9O6RHnsu3up"
      },
      "source": [
        "Then we loop the function over the first 10 images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euKNnitcu8ul"
      },
      "source": [
        "options(repr.plot.width = 16, repr.plot.height = 8)\n",
        "\n",
        "# Set the layout\n",
        "layout(matrix(1:10, ncol = 5))\n",
        "\n",
        "# Loop plotting over the batch of images\n",
        "for(i in 1:10){\n",
        "  plot_rgb_image_ttl(pred_batch[[1]][i,,,], predictions[i,])\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPQYQLx3cYq"
      },
      "source": [
        "# Part 3: Save as Keras `.h5` model\n",
        "\n",
        "Now that we've trained the model,  we can save it as an HDF5 file, which is the format used by Keras using function [`save_model_hdf5()`](https://keras.rstudio.com/reference/save_model_hdf5.html). Our HDF5 file will have the extension '.h5', and it's name will correpond to the current time stamp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnNWTkZ3Ckz"
      },
      "source": [
        "export_path_keras = paste0(\"./\", as.integer(Sys.time()) ,\".h5\")\n",
        "export_path_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgL20byNv6Ii"
      },
      "source": [
        "model %>% save_model_hdf5(filepath = export_path_keras)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi3YWnl51vMU"
      },
      "source": [
        "fs::dir_tree(recurse = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u6BiynOxs6z"
      },
      "source": [
        " You can later recreate the same model from this file, even if you no longer have access to the code that created the model.\n",
        "\n",
        "This file includes:\n",
        "\n",
        "- The model's architecture\n",
        "- The model's weight values (which were learned during training)\n",
        "- The model's training config (what you passed to `compile`), if any\n",
        "- The optimizer and its state, if any (this enables you to restart training where you left off)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QMVDmgAx3-L"
      },
      "source": [
        "# Part 4:  Load the Keras `.h5` Model\n",
        "\n",
        "We will now load the model we just saved into a new model called `reloaded`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXFwZsFQyCfy"
      },
      "source": [
        "reloaded_model <- load_model_hdf5(export_path_keras, \n",
        "                                  custom_objects = list(loss_sparse_categorical_crossentropy = loss_sparse_categorical_crossentropy))\n",
        "                    \n",
        "reloaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX_EiYB42s_k"
      },
      "source": [
        "We can check that the reloaded model and the previous model give the same result. Let's make some predictions using the reloaded model on the batch of images we had previously used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcIS51Vx2RUc"
      },
      "source": [
        "pred_mat_reloaded <- reloaded_model %>%\n",
        "  predict(pred_batch[[1]]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBmRjIdN279b"
      },
      "source": [
        "Predicted classes from both models should be equal so the following statement should return `TRUE`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C-lVrNi2bru"
      },
      "source": [
        "all(apply(pred_mat, 1, which.max) == apply(pred_mat_reloaded, 1, which.max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGqic8G33KtH"
      },
      "source": [
        "# Keep Training\n",
        "\n",
        "Besides making predictions, we can also take our `reloaded_model` and keep training it. To do this, you can just train the `reloaded_model` as usual, using the `fit_generator` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T38yNY0i3VKn"
      },
      "source": [
        "epochs = 3\n",
        "history <- reloaded_model %>%\n",
        "              fit_generator(generator = train_data_gen,\n",
        "              steps_per_epoch = as.integer(total_train / batch_size),\n",
        "              epochs = epochs,\n",
        "              validation_data = val_data_gen,\n",
        "              validation_steps = as.integer(total_val / batch_size))\n",
        "\n",
        "cat('Validation loss:', format(tail(history$metrics$val_loss, 1), digits = 2), \"\\n\")\n",
        "cat('Validation accuracy:', format(tail(history$metrics$val_accuracy, 1), digits = 2), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}