{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R04_C01_dogs_vs_cats_with_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1z4xy2gTUc4a"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Notice\n",
        "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
        "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
        "*   Edit -> Clear all outputs\n",
        "*   Runtime -> Reset all runtimes"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Lab 04b: Dogs vs Cats Image Classification With Image Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R04_C02_dogs_vs_cats_with_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R04_C02_dogs_vs_cats_with_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "In this tutorial, we will discuss how to classify images into pictures of cats or pictures of dogs. We'll build an image classifier using a `keras_model_sequential()` model and load data using `keras` function `image_data_generator()`.\n",
        "\n",
        "## Specific concepts that will be covered:\n",
        "In the process, we will build practical experience and develop intuition around the following concepts\n",
        "\n",
        "* Building _data input pipelines_ using the `image_data_generator()` class â€” How can we efficiently work with data on disk to interface with our model?\n",
        "* _Overfitting_ - what is it, how to identify it, and how can we prevent it?\n",
        "* _Data Augmentation_ and _Dropout_ - Key techniques to fight overfitting in computer vision tasks that we will incorporate into our data pipeline and image classifier model.\n",
        "\n",
        "## We will follow the general machine learning workflow:\n",
        "\n",
        "1. Examine and understand data\n",
        "2. Build an input pipeline\n",
        "3. Build our model\n",
        "4. Train our model\n",
        "5. Test our model\n",
        "6. Improve our model/Repeat the process\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Before you begin**\n",
        "\n",
        "Before running the code in this notebook, reset the runtime by going to **Runtime -> Reset all runtimes** in the menu above. If you have been working through several notebooks, this will help you avoid reaching Colab's memory limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHx57rQES5y0"
      },
      "source": [
        "# Install and load dependencies\n",
        "\n",
        "First, you'll need to install and load R package Keras which will also install TensorFlow. We'll also install package `fs` which has useful functionality for working with our filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvHtCuB1TNHd"
      },
      "source": [
        "install.packages(c(\"keras\", \"fs\"))\n",
        "library(keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHx8-t-VrVo"
      },
      "source": [
        "To build our image classifier, we begin by downloading the dataset. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n",
        "\n",
        "In previous Colabs, we've used datasets available through the `Keras` package, which is a very easy and convenient way to use datasets. In this Colab however, we will make use of the class `image_data_generator()` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to the Colab filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP10Mi8KBnZT"
      },
      "source": [
        "URL <- \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "zip_dir <- get_file('cats_and_dogs_filtered.zip', origin = URL, extract = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqy73uxUDpTl"
      },
      "source": [
        "The dataset we have downloaded has the following directory structure.\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>\n",
        "\n",
        "We can list the directories with the following terminal command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm1mzco4Ecfa"
      },
      "source": [
        "zip_dir_base <- dirname(zip_dir)\n",
        "fs::dir_tree(zip_dir_base, recurse = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hy-Et8FiLX"
      },
      "source": [
        "We'll now assign variables with the proper file path for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJosGnjvFjqJ"
      },
      "source": [
        "base_dir <- fs::path(zip_dir_base, \"cats_and_dogs_filtered\")\n",
        "train_dir <- fs::path(base_dir, \"train\")\n",
        "validation_dir <- fs::path(base_dir, \"validation\")\n",
        "\n",
        "train_cats_dir <- fs::path(train_dir, \"cats\")\n",
        "train_dogs_dir <- fs::path(train_dir, \"dogs\")\n",
        "validation_cats_dir <- fs::path(validation_dir, \"cats\")\n",
        "validation_dogs_dir <- fs::path(validation_dir, \"dogs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdrHHTy2VrV3"
      },
      "source": [
        "### Understanding our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LblUYjl-VrV3"
      },
      "source": [
        "Let's look at how many cats and dogs images we have in our training and validation directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GSeM85GC6Ar"
      },
      "source": [
        "num_cats_tr <- length(fs::dir_ls(train_cats_dir))\n",
        "num_dogs_tr <- length(fs::dir_ls(train_dogs_dir))\n",
        "\n",
        "num_cats_val <- length(fs::dir_ls(validation_cats_dir))\n",
        "num_dogs_val <- length(fs::dir_ls(validation_dogs_dir))\n",
        "\n",
        "total_train <- num_cats_tr + num_dogs_tr\n",
        "total_val <- num_cats_val + num_dogs_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njcNKT-_Djw9"
      },
      "source": [
        "cat('total training cat images:', num_cats_tr, \"\\n\")\n",
        "cat('total training dog images:', num_dogs_tr, \"\\n\")\n",
        "\n",
        "cat('total validation cat images:', num_cats_val, \"\\n\")\n",
        "cat('total validation dog images:', num_dogs_val, \"\\n\")\n",
        "cat(\"--\", \"\\n\")\n",
        "cat(\"Total training images:\", total_train, \"\\n\")\n",
        "cat(\"Total validation images:\", total_val, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdsI_L-NVrV_"
      },
      "source": [
        "# Setting Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-0ejxOtP1"
      },
      "source": [
        "For convenience, we'll set up variables that will be used later while pre-processing our dataset and training our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NqNselLVrWA"
      },
      "source": [
        "batch_size = 100  # Number of training examples to process before updating our models variables\n",
        "img_shape  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INn-cOn1VrWC"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voLJVUaBvvOm"
      },
      "source": [
        "Overfitting often occurs when we have a small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number and variety of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples through random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This exposes the model to more aspects of the data, allowing it to generalize better.\n",
        "\n",
        "In **keras** we can implement this using the same **`image_data_generator`** class we used before. We can simply pass different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process.\n",
        "\n",
        "To start off, let's define a function that can display an image, so we can see the type of augmentation that has been performed. Then, we'll look at specific augmentations that we'll use during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0qapJ_aIq3v"
      },
      "source": [
        "plot_rgb_image <- function(image_array){\n",
        "  image_array %>%\n",
        "  array_reshape(dim = c(dim(.)[1:3])) %>%\n",
        "  as.raster(max = 1) %>%\n",
        "  plot()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlVj6VqaVrW0"
      },
      "source": [
        "### Flipping the image horizontally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcdvx4TVVrW1"
      },
      "source": [
        "We can begin by randomly applying horizontal flip augmentation to our dataset and seeing how individual images will look after the transformation. This is achieved by passing `horizontal_flip = TRUE` as an argument to the `image_data_generator` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ofTrM6XJWmf"
      },
      "source": [
        "train_image_generator <- image_data_generator(rescale = 1/255,\n",
        "                                              horizontal_flip = TRUE)\n",
        "\n",
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fTL9g1UJwHh"
      },
      "source": [
        "To see the transformation in action, let's take one sample image from our training set and repeat it four times. The augmentation will be randomly applied (or not) to each repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL7G7hqELS4o"
      },
      "source": [
        "n <- 4\n",
        "augmented_training_images <- list(length = n)\n",
        "for(i in 1:n){\n",
        " augmented_training_images[[i]] <- train_data_gen[1][[1]][1,,,]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBk8J3YaJxl0"
      },
      "source": [
        "options(repr.plot.width = 16, repr.plot.height = 4)\n",
        "\n",
        "# Loop plotting over n images\n",
        "layout(matrix(1:n, ncol = 4))\n",
        "\n",
        "for(i in 1:n){\n",
        "  augmented_training_images[[i]] %>%\n",
        "  plot_rgb_image()\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7n9xcqCVrXB"
      },
      "source": [
        "### Rotating the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXnwkzFuVrXB"
      },
      "source": [
        "The rotation augmentation will randomly rotate the image up to a specified number of degrees. Here, we'll set it to 45."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zip35pDVrXB"
      },
      "source": [
        "train_image_generator <- image_data_generator(rescale = 1/255,\n",
        "                                              rotation_range = 45)\n",
        "\n",
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deaqZLsfcZ15"
      },
      "source": [
        "To see the transformation in action, let's once again take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVoWh4OIVrXD"
      },
      "source": [
        "layout(matrix(1:n, ncol = 4))\n",
        "for(i in 1:n){\n",
        " train_data_gen[1][[1]][1,,,] %>%\n",
        "  plot_rgb_image()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOqGPL76VrXM"
      },
      "source": [
        "### Applying Zoom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvqXaD8BVrXN"
      },
      "source": [
        "We can also apply Zoom augmentation to our dataset, zooming images up to 50% randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGNKLa_YVrXR"
      },
      "source": [
        "train_image_generator <- image_data_generator(rescale = 1/255,\n",
        "                                              zoom_range = 0.5)\n",
        "\n",
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPWieSZcctO"
      },
      "source": [
        "One more time, take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOvTs32FVrXU"
      },
      "source": [
        "layout(matrix(1:n, ncol = 4))\n",
        "for(i in 1:n){\n",
        " train_data_gen[1][[1]][1,,,] %>%\n",
        "  plot_rgb_image()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usS13KCNVrXd"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC8fIsalVrXd"
      },
      "source": [
        "We can apply all these augmentations, and even others, with just one line of code, by passing the augmentations as arguments with proper values.\n",
        "\n",
        "Here, we have applied rescale, rotation of 45 degrees, width shift, height shift, horizontal flip, and zoom augmentation to our training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc4vEtG1SeSt"
      },
      "source": [
        "train_image_generator <- image_data_generator(rescale = 1/255,\n",
        "                                              rotation_range = 45,\n",
        "                                              width_shift_range = 0.2,\n",
        "                                              height_shift_range = 0.2,\n",
        "                                              shear_range = 0.2,\n",
        "                                              zoom_range = 0.2,\n",
        "                                              horizontal_flip = TRUE,\n",
        "                                              fill_mode = 'nearest')\n",
        "\n",
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpleAdUIS4EF"
      },
      "source": [
        "Let's visualize how a single image would look like four different times, when we pass these augmentations randomly to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK7UFZQNS3YM"
      },
      "source": [
        "layout(matrix(1:n, ncol = 4))\n",
        "for(i in 1:n){\n",
        " train_data_gen[1][[1]][1,,,] %>%\n",
        "  plot_rgb_image()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8cUd7FXVrXq"
      },
      "source": [
        "### Creating Validation Data generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99fDBt7VrXr"
      },
      "source": [
        "Generally, we only apply data augmentation to our training examples, since the original images should be representative of what our model needs to manage. So, in this case we are only rescaling our validation images and converting them into batches using `image_data_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQnAVzfjTPD-"
      },
      "source": [
        "val_image_generator <- image_data_generator(rescale = 1/255)\n",
        "\n",
        "val_data_gen <- flow_images_from_directory(directory = validation_dir,\n",
        "                                             generator = val_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mccwi4LuLhJ"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "The Droput (`layer_dropout`) layer can be used to help prevent overfitting by randomly 'dropping out' the previous layer's outputs. The first parameter of the layer is the `rate`, a percentage of previous layer's outputs that will be dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0lN5NRPT5xf"
      },
      "source": [
        "```r\n",
        "# An example of how dropout is placed in a sequence of layers\n",
        "layer_conv_2d(64, (3,3), activation='relu') %>%\n",
        "layer_max_pooling_2d(2,2) %>%\n",
        "layer_dropout(0.2), # Dropout layer added with a rate of 0.2\n",
        "layer_conv_2d(64, (3,3), activation='relu') %>%\n",
        "layer_max_pooling_2d(2,2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ej-HLGVrWZ"
      },
      "source": [
        "# Exercise 4.4 Model Creation (15 mins) + break for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4KEzz51uLhJ"
      },
      "source": [
        "Use what you've learned to create a CNN model, train it on the image generator we've created and visualise the results. You'll need to:\n",
        "\n",
        "* **Define your model** with the the following layers. Add a `Dropout` layer with the rate of `0.5` after the last pooling layer:\n",
        "    * 2D Convolution - `img_shape` x `img_shape` x 3 input shape, 32 filters, 3x3 kernel, ReLU activation\n",
        "    * 2D Max pooling - 2x2 kernel\n",
        "    * 2D Convolution - 64 filters, 3x3 kernel, ReLU activation\n",
        "    * 2D Max pooling - 2x2 kernel\n",
        "    * 2D Convolution - 128 filters, 3x3 kernel, ReLU activation\n",
        "    * 2D Max pooling - 2x2 kernel\n",
        "    * 2D Convolution - 128 filters, 3x3 kernel, ReLU activation\n",
        "    * 2D Max pooling - 2x2 kernel\n",
        "    * **Dropout - rate 0.5**\n",
        "    * Flatten\n",
        "    * Dense - 512 nodes, ReLU activation\n",
        "    * Dense - 2 nodes, softmax activation \n",
        "* **Compile the model**\n",
        "    * Use the `adam` optimizer, `sparse_categorial_crossentropy` loss and get the `accuracy` metric.\n",
        "* Print out a summary of your model (optional)\n",
        "* **Train the model** on the created image generators over 40 epochs\n",
        "* **Print out and plot the output statistics** (`history`)\n",
        "\n",
        "Check the documentation if you've forgotten any steps  [https://keras.rstudio.com/reference/index.htm](https://keras.rstudio.com/reference/index.htm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08rRJ0sn3Tb1"
      },
      "source": [
        "# TODO - Create, compile, and train the model. Then visualise the results.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "skCNqZxCuLhK"
      },
      "source": [
        "## Exercise 4.4 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.4.4.ipynb)\n",
        "\n"
      ]
    }
  ]
}