{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R06_saving_and_loading_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVGashPhAkOa"
      },
      "source": [
        "# split datasets\n",
        "c(train_images, train_labels) %<-% cifar10$train\n",
        "c(test_images, test_labels) %<-% cifar10$test\n",
        "\n",
        "# pre-process images\n",
        "train_images <- train_images/255\n",
        "test_images <- test_images/255\n",
        "\n",
        "# create model\n",
        "model <- keras_model_sequential()\n",
        "model %>% \n",
        "  layer_conv_2d(filters = 32, kernel_size = c(3,3), padding = 'same', \n",
        "    activation = 'relu', input_shape = c(32, 32, 3)) %>%\n",
        "  layer_max_pooling_2d(pool_size = c(2, 2), strides = 2) %>%\n",
        "  layer_conv_2d(filters = 64, kernel_size = c(3,3), padding = 'same', \n",
        "    activation = 'relu') %>% \n",
        "  layer_max_pooling_2d(pool_size = c(2, 2)) %>%   \n",
        "  layer_flatten() %>% \n",
        "  layer_dense(units = 128, activation = 'relu') %>% \n",
        "  layer_dense(units = 10, activation = 'softmax')\n",
        "\n",
        "  # compile model\n",
        "model %>% compile(optimizer = 'adam', \n",
        "                  loss = 'sparse_categorical_crossentropy',\n",
        "                  metrics = c('accuracy'))\n",
        "\n",
        "# fit model\n",
        "model %>% fit(train_images, train_labels, epochs = 5)\n",
        "\n",
        "# evaluate model\n",
        "score <- model %>% evaluate(test_images, test_labels)\n",
        "cat('Test loss:', score[\"loss\"], \"\\n\")\n",
        "cat('Test accuracy:', score[\"accuracy\"], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}